import streamlit as st
from utils.embedding_utils import get_embedder, format_query_e5
from utils.index_utils import load_index_bundle
from sentence_transformers import CrossEncoder
import ollama
from openai import OpenAI


def inferir_tipos_relevantes(pergunta):
    pergunta = pergunta.lower()
    tipos = []

    if "objetivo espec√≠fico" in pergunta or "objetivos espec√≠ficos" in pergunta:
        tipos.append("objetivo_especifico")
    if "objetivo geral" in pergunta:
        tipos.append("objetivo_geral")
    if "objetivo estrat√©gico" in pergunta or "objetivos estrat√©gicos" in pergunta:
        tipos.append("objetivo_estrategico")
    if "problema" in pergunta:
        tipos.append("problema")
    if "causa" in pergunta:
        tipos.append("causa")
    if "justificativa" in pergunta:
        tipos.append("justificativa")
    if "evid√™ncia" in pergunta or "evidencias" in pergunta:
        tipos.append("evidencia")
    if "entrega" in pergunta or "entregas" in pergunta:
        tipos.append("entrega")
    if "p√∫blico-alvo" in pergunta or "p√∫blico alvo" in pergunta:
        tipos.append("publico_alvo")

    return tipos or None


def handle_chat(options):
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Renderiza mensagens anteriores
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Entrada do usu√°rio
    if prompt := st.chat_input("Fa√ßa sua pergunta sobre os programas:"):
        retrieved_chunks = []

        if not options["selected_indices"]:
            st.warning("Selecione pelo menos um √≠ndice.")
            return

        embedder = get_embedder(options['embedding_model'])
        query = format_query_e5(prompt) if options['embedding_model'] == "multilingual_e5_large" else prompt

        for idx in options['selected_indices']:
            index = load_index_bundle(idx, embedder)
            results = index.similarity_search(query, k=options["top_k_retrieval"])
            retrieved_chunks.extend(results)  # Mant√©m como `Document` para preservar metadados

        # Filtro por tipo, baseado na pergunta
        tipos_desejados = inferir_tipos_relevantes(prompt)
        if tipos_desejados:
            retrieved_chunks = [
                doc for doc in retrieved_chunks
                if doc.metadata.get("tipo") in tipos_desejados
            ]

        if not retrieved_chunks:
            st.warning("Nenhum chunk relevante encontrado com base nos filtros aplicados.")
            return

        # Apresenta os chunks recuperados inicialmente
        with st.expander("üîé Chunks Recuperados (antes do reranking)", expanded=False):
            for i, doc in enumerate(retrieved_chunks, 1):
                st.markdown(f"**Chunk {i}:**")
                st.markdown(f"`Metadados:` `{doc.metadata}`")
                st.markdown(f"```\n{doc.page_content}\n```")

        # Rerank
        cross_encoder = CrossEncoder(options['reranker_model'])
        pairs = [[prompt, doc.page_content] for doc in retrieved_chunks]
        scores = cross_encoder.predict(pairs)
        ranked = sorted(zip(scores, retrieved_chunks), reverse=True)
        reranked_chunks = [doc.page_content for score, doc in ranked[:options['top_k_rerank']]]

        # Mostrar ap√≥s rerank
        with st.expander("üèÜ Chunks Selecionados ap√≥s Reranking", expanded=True):
            for i, (score, doc) in enumerate(ranked[:options['top_k_rerank']], 1):
                st.markdown(f"**#{i} ‚Äî Score: {score:.4f}**")
                st.markdown(f"`Metadados:` `{doc.metadata}`")
                st.markdown(f"```\n{doc.page_content}\n```")

        context = "\n\n".join(reranked_chunks)

        # Prompt do sistema
        system_prompt = (
            "Voc√™ √© um assistente especializado na an√°lise de documentos de planejamento p√∫blico, com acesso a trechos "
            "documentos relativos ao Plano Plurianual (PPA).\n"
            "Seu trabalho √© responder com base **exclusivamente no conte√∫do abaixo**, sem usar conhecimento externo ou fazer suposi√ß√µes.\n\n"
            f"üìÑ **Trechos do documento (contexto):**\n{context}\n\n"
            f"‚ùì **Pergunta:**\n{prompt}\n\n"
            "üìå **Instru√ß√µes de resposta**:\n"
            "- Utilize os metadados dos chunks para identificar claramente o tipo de informa√ß√£o (ex: objetivo espec√≠fico, problema, justificativa...)\n"
            "- Liste sempre todos os que se referem ao mesmo programa, sem exce√ß√£o.\n"
            "- Se a resposta estiver nos trechos, **repita exatamente a mesma reda√ß√£o**.\n"
            "- Nunca misture conceitos como objetivo geral, estrat√©gico e espec√≠fico.\n"
            "- Ignore qualquer informa√ß√£o fora dos trechos. Responda com base **exclusiva** no que foi extra√≠do.\n"
            "üîÅ Agora responda:"
        )

        # Escolha do modelo
        llm = options["llm_choice"]

        if llm == "GPT-4":
            client = OpenAI()
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": context + "\n\n" + prompt}
                ],
                temperature=options["temperature"]
            )
            response_text = response.choices[0].message.content

        elif llm == "Ollama":
            response = ollama.chat(
                model="llama3",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": context + "\n\n" + prompt}
                ]
            )
            response_text = response["message"]["content"]

        elif llm == "Copilot":
            response_text = "‚ö†Ô∏è O modelo Copilot ainda n√£o foi implementado."

        # Mostra resposta
        with st.chat_message("assistant"):
            st.markdown(response_text)

        # Hist√≥rico
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.session_state.messages.append({"role": "assistant", "content": response_text})
