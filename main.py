import streamlit as st
from config.settings import INDEX_DIR, DATA_FOLDER
from utils.index_utils import save_index_bundle
from utils.embedding_utils import get_embedder
from components.sidebar import show_sidebar
from components.chat_handler import handle_chat
from langchain_core.documents import Document
from utils.process_pdf import process_pdf_to_jsonl
from utils.file_utils import read_jsonl_files


# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(page_title="Pergunte ao PPA", layout="wide")

# UI lateral
options = show_sidebar()

uploaded_files = st.file_uploader("Carregar CSVs", type="csv", accept_multiple_files=True) if options['data_source'] == "Upload manual" else None

# IndexaÃ§Ã£o
uploaded_pdf = st.file_uploader("Carregar PDF do programa", type="pdf")


# UI para upload
uploaded_pdf = st.file_uploader("Carregar PDF do programa", type="pdf", key="pdf_upload_main") if options["data_source"] == "Upload manual" else None

if st.button("Iniciar IndexaÃ§Ã£o"):
    try:
        with st.spinner("Processando e indexando..."):

            # ğŸ“ Escolha do caminho do PDF
            if options["data_source"] == "Pasta padrÃ£o":
                pdf_path = "fonte_de_dados/Espelho_SIOP_1752869188991.pdf"
                st.write(f"ğŸ“ Usando PDF local salvo: `{pdf_path}`")
            elif uploaded_pdf is not None:
                pdf_path = f"/tmp/{uploaded_pdf.name}"
                with open(pdf_path, "wb") as f:
                    f.write(uploaded_pdf.read())
                st.write(f"ğŸ“¤ PDF recebido por upload: `{pdf_path}`")
            else:
                st.warning("âš ï¸ VocÃª deve carregar um PDF.")
                st.stop()

            # ğŸ“„ Gera JSONL com chunks do PDF
            st.write("ğŸ“¦ Extraindo informaÃ§Ãµes e gerando chunks...")
            jsonl_path, total_chunks = process_pdf_to_jsonl(
                pdf_path,
                "fonte_de_dados/dados_abertos/programas_chunked_tokenlimit_300.jsonl",
                token_limit=options["chunk_size"]
            )

            st.write(f"âœ… JSONL gerado: `{jsonl_path}` com `{total_chunks}` chunks")

            # ğŸ“š Carregar os dados do JSONL
            #raw_texts = read_jsonl_files([jsonl_path], options["chunk_size"], options["embedding_model"])
            raw_texts = read_jsonl_files(["fonte_de_dados/dados_abertos/programas_chunked_tokenlimit_300_jupiter.jsonl"], options["chunk_size"], options["embedding_model"])

            # ğŸ” Log de debug dos primeiros chunks
            st.write("ğŸ” VisualizaÃ§Ã£o dos primeiros chunks:")
            for i, item in enumerate(raw_texts[:5]):
                st.markdown(f"**Chunk {i+1}**")
                st.markdown(f"`campos_presentes:` `{item['metadata'].get('campos_presentes')}`")
                st.code(item["text"][:500] + ("..." if len(item["text"]) > 500 else ""))

            documents = [
                Document(page_content=doc["text"], metadata=doc["metadata"])
                for doc in raw_texts
            ]

            st.write(f"ğŸ§  Total de documentos convertidos para indexaÃ§Ã£o: {len(documents)}")

            # ğŸ’¾ Embedding e indexaÃ§Ã£o
            embedder = get_embedder(options["embedding_model"])
            index_name = f"{options['embedding_model']}_{options['index_version']}"
            save_index_bundle(documents, embedder, index_name)

            st.success(f"ğŸ¯ IndexaÃ§Ã£o concluÃ­da com sucesso: `{index_name}`")
            st.write(f"ğŸ“Š Total de chunks indexados: {len(documents)}")
            st.write(f"ğŸ§· Campos detectados nos primeiros metadados: {[doc.metadata.get('campos_presentes') for doc in documents[:5]]}")

    except Exception as e:
        st.error(f"âŒ Erro durante a indexaÃ§Ã£o: {e}")
        import traceback
        st.code(traceback.format_exc())



# Chat
handle_chat(options)
